---
title: "POLI3115 gp code: sentiment analysis and topic modeling"
author: "Yvonne JIN"
date: "4/13/2022"
output: html_document
---

```{r setup, include=FALSE}
library(jiebaR)
library(lubridate)
library(dplyr)
library(tidyr)
library(readr)
library(ggplot2)
```

Data import and formating

```{r eval=FALSE, include=FALSE}
df <- read_csv("sampled_0414.csv")

# turn off scientific notation for variable "weibo_id"
df$weibo_id <- format(df$weibo_id, scientific = FALSE)

# dealing with timestamp
# conversion of timestamp variable into POSIXct format
df$timestamp <- as.POSIXct(df$timestamp, format = "%Y-%m-%d %H:%M:%S")
# extract date from timestamp
df$date <- format(df$timestamp, format = "%Y-%m-%d")
# calculate no. of post per day
date_post_n <- df %>%
  group_by(date) %>%
  summarise(npost = n())

# take a look at the dataset
head(df)
dim(df)
```

# Text Tokenization

```{r cars, eval=FALSE, include=FALSE}

#Separate tokenized text into documents
mixseg <- worker()
# remove "." at the end of sentences (which causes error when stemming)
df$content=gsub("\\.","",df$content)
text_seg <- lapply(df$content, function(x) segment(x, mixseg))

# Tokenize each post. Keep weibo id and timestamp as identifiers
df_seg <- df %>%
  select(timestamp, user_id, weibo_id) %>%
  mutate(word = text_seg) %>%
  unnest("word")

# cleaning 1 unit words ("?" -> sentiment?)
# 2268k -> 1579k 
df_seg <- df_seg %>% filter(nchar(word)>1)

```

```{r eval=FALSE, include=FALSE}
# save segmented dataset 
#save.image("samplecontent_stemmed.RData")
# save segmented word data
saveRDS(df_seg, "sample2/content_stemmed.rds")
```

# Sentiment analysis

```{r eval=FALSE, include=FALSE}


# load stemmed word list and 
df_seg <- readRDS("sample2/content_stemmed.rds")

## Finalized dictionary for news documents  ----
# text frequency by each poster
text_count <- df_seg %>%
  group_by(timestamp,user_id, word) %>%
  count()
# save word frequency table 
saveRDS(text_count,"sample2/word_freq.rds")

# text frequency in total： 96k distinctive words
dictionary_t <- text_count %>%
  group_by(word) %>%
  summarise(
    term_frequency = sum(n), 
    document_frequency = n())

```

## Sentiment analysis by each post

```{r}

library("glue")
library("stringr")
# loading chinese emotion dictionary
library(readxl)

text_count <- readRDS("sample2/word_freq.rds")

# load and process the chinese emotion library
Emo_dic_chi <- read_excel("情感词汇本体.xlsx")
colnames(Emo_dic_chi) <- c("word","PoS","meaningN","meaningn",
                           "emotion","valence","polarity",
                           "peripheral_emo","valence_p","polarity_p")
summary(Emo_dic_chi)
Emo_dic_chi <- Emo_dic_chi[,1:10] #drop NA columns
head(Emo_dic_chi)

# conversion of timestamp variable into
# POSIXct format
text_count$timestamp <- as.POSIXct(text_count$timestamp, format = "%Y-%m-%d %H:%M:%S")
# extract date from timestamp
text_count$date <- format(text_count$timestamp, format = "%Y-%m-%d")

## by emotion type 
emo_post_count <- text_count %>%
  inner_join(Emo_dic_chi, by = "word") %>%
  group_by(timestamp,user_id, emotion) %>%
  summarise(score = sum(n)) %>%
  pivot_wider(names_from = "emotion", values_from = "score", 
              values_fill = 0, names_prefix = "emo_chi_count_")

## by sentiment polarity 
sent_post_count <- text_count %>%
  inner_join(Emo_dic_chi, by = "word") %>%
  group_by(timestamp,user_id, polarity) %>%
  summarise(score = sum(n)) %>%
  pivot_wider(names_from = "polarity", values_from = "score", 
              values_fill = 0, names_prefix = "sent_chi_count_")

# conversion of timestamp to date
emo_post_count$timestamp <- as.POSIXct(emo_post_count$timestamp, format = "%Y-%m-%d %H:%M:%S")
emo_post_count$date <- format(emo_post_count$timestamp, format = "%Y-%m-%d")

sent_post_count$timestamp <- as.POSIXct(sent_post_count$timestamp, format = "%Y-%m-%d %H:%M:%S")
sent_post_count$date <- format(sent_post_count$timestamp, format = "%Y-%m-%d")

```

## Sentiment analysis by each day

```{r}
# conversion of timestamp variable into
# POSIXct format
text_count$timestamp <- as.POSIXct(text_count$timestamp, format = "%Y-%m-%d %H:%M:%S")
# extract date from timestamp
text_count$date <- format(text_count$timestamp, format = "%Y-%m-%d")

## by emotion type
emo_date_count <- text_count %>%
  inner_join(Emo_dic_chi, by = "word") %>%
  group_by(date, emotion) %>%
  summarise(score = sum(n)) %>%
  pivot_wider(names_from = "emotion", values_from = "score", 
              values_fill = 0, names_prefix = "emo_chi_count_")

## by sentiment polarity 
sent_date_count <- text_count %>%
  inner_join(Emo_dic_chi, by = "word") %>%
  group_by(date, polarity) %>%
  summarise(score = sum(n)) %>%
  pivot_wider(names_from = "polarity", values_from = "score", 
              values_fill = 0, names_prefix = "sent_chi_count_")

```

## Visualisation

```{r}

# add no. of posts info per day
sent_date_count <- sent_date_count %>% full_join(date_post_n, by = "date")
# calculate sentiment index
sent_date_count <- mutate(sent_date_count, 
                         sent_index = sent_chi_count_1 - sent_chi_count_2)

write_csv(sent_date_count,"sent_by_date.csv")

g_sent_index <- ggplot(data = sent_date_count, aes(date, sent_index/npost)) +
  geom_point() + 
  geom_line() + 
  scale_x_discrete(guide = guide_axis(check.overlap = TRUE)) + 
  labs(x = "Date", y = "Sentiment", 
       title = "Diffrence between positive and negative sentiment / No. of posts per day") +
  theme_minimal()
  
g_sent_index

# calculate sentiment index per post
sent_post_count <- mutate(sent_post_count, 
                         sent_index = sent_chi_count_1 - sent_chi_count_2)
# scatter plot
sent_index_scatter <- ggplot(data = sent_post_count, aes(x = date, y = sent_index)) +
  geom_point(alpha = 1/3, size = 1,colour = "royalblue4") + 
  geom_line(size = 0.1) +
  scale_x_discrete(guide = guide_axis(check.overlap = TRUE))
  #theme_minimal()
# change dot color 
colors()

sent_index_scatter
  

  
ggsave(file="sentiment_by_post.png", width=7.5, height=4, dpi=300)

# https://ggplot2.tidyverse.org/articles/ggplot2-specs.html?q=lines#lines



# find max and min of each day
sent_post_maxmin <- sent_post_count %>% group_by(date) %>%
  summarise(index_max = max(sent_index, na.rm=TRUE),
             index_min = min(sent_index, na.rm=TRUE)) %>%
  pivot_longer(cols = index_max:index_min,
               names_to = "extremes",
               values_to = "index")

ggplot(data = sent_post_maxmin, aes(date,index) ) +
  geom_point() +
  geom_line() +
  scale_x_discrete(guide = guide_axis(check.overlap = TRUE))

g_sent_post_index

ggsave(file="sentiment_by_post.png", width=8, height=4, dpi=300)

```


```{r}

# emotion type

# add no. of posts info per day
emo_date_count <- emo_date_count %>% full_join(date_post_n, by = "date")

# count of sentiment polarity divided by no. of posts per day
sent_polarity <- ggplot(data = sent_date_count, aes(x = date)) +
  geom_point(aes(y = sent_chi_count_0/npost, color = "neutral"))+
  geom_point(aes(y = sent_chi_count_1/npost, color = "positive"))+
  geom_point(aes(y = sent_chi_count_2/npost, color = "negative")) + 
  scale_x_discrete(guide = guide_axis(check.overlap = TRUE)) + 
  labs(x = "Date", y = "Sentiment",
       title = "Sentiment Polarity / No. of posts per day") + 
  theme_minimal()
sent_polarity

# scatter plot by post
sent_post_polarity <- ggplot(data = sent_post_count, aes(x = date)) +
  geom_point(aes(y = sent_chi_count_0, color = "neutral"))+
  geom_point(aes(y = sent_chi_count_1, color = "positive"))+
  geom_point(aes(y = sent_chi_count_2, color = "negative")) + 
  scale_x_discrete(guide = guide_axis(check.overlap = TRUE)) + 
  labs(x = "Date", y = "Sentiment",
       title = "Sentiment Polarity") + 
  theme_minimal()
sent_post_polarity

```


## After COVID outbreak
```{r}
onset <- as.POSIXct("2019-12-08", format = "%Y-%m-%d") # debatable!!
sent_after_COVID <- sent_date_count %>% filter(date>onset) %>%
  mutate(sent_index = sent_chi_count_1 - sent_chi_count_2)
# point plot
sent_polarity_COVID <- ggplot(data = sent_after_COVID, aes(x = date)) +
  geom_point(aes(y = sent_chi_count_0/npost, color = "neutral"))+
  geom_point(aes(y = sent_chi_count_1/npost, color = "positive"))+
  geom_point(aes(y = sent_chi_count_2/npost, color = "negative")) + 
  scale_x_discrete(guide = guide_axis(check.overlap = TRUE)) +
  labs(x = "Date", y = "Sentiment",
       title = "Sentiment Polarity / No. of posts per day") + 
  theme_minimal()
sent_polarity_COVID
# sentiment index
sent_index_COVID <- ggplot(data = sent_after_COVID,mapping =  aes(x = date, y = sent_index/npost,group = 1)) +
  geom_point() + 
  geom_line() +
  scale_x_discrete(guide = guide_axis(check.overlap = TRUE))+
  labs(x = "Date", y = "Sentiment", 
       title = "Diffrence between positive and negative sentiment / No. of posts per day") +
  theme_minimal() + 
  geom_hline(yintercept = 0, linetype="dotted", size = 0.3)
sent_index_COVID

## by each post
onset <- as.POSIXct("2019-12-08", format = "%Y-%m-%d") # debatable!!
sent_post_COVID <- sent_post_count %>% filter(date>onset) %>%
  mutate(sent_index = sent_chi_count_1 - sent_chi_count_2)

# scatter plot
sent_index_scatter <- ggplot(data = sent_post_COVID, aes(x = date, y = sent_index)) +
  geom_point(alpha = 1/3, size = 1,colour = "royalblue4") + 
  geom_line(size = 0.1) +
  scale_x_discrete(guide = guide_axis(check.overlap = TRUE))
  #theme_minimal()
# change dot color 
colors()

sent_index_scatter


  
ggsave(file="sentiment_by_post_COVID.png", width=7.5, height=4, dpi=300)

# https://ggplot2.tidyverse.org/articles/ggplot2-specs.html?q=lines#lines




```

```{r}
#save.image("sample2/sentiment_analysis_viz.Rdata")
saveRDS(emo_date_count, "sample2/emotion_categorization_by_count.rds")
saveRDS(sent_date_count, "sample2/sentiment_categorization_by_count.rds")
saveRDS(date_post_n,"sample2/npost_bydate.rds")
```

## Add key timepoints to graph

```{r}

Time <- c("2019-12-01","2020-01-23","2020-01-27",
                   "2020-02-06","2020-02-13","2020-02-25",
                   "2020-03-10")
Event <- c("First Wuhan Patient",
                    "Lockdown of Wuhan",
                    "Li Keqiang visited Wuhan",
                    "Death of Li Wenliang",
                    "Replacement of Wuhan Communist Party chief",
                    "Partial lockdown of Italy",
                    "Xi visited Wuhan")
timeline <- data.frame(Time = Time, Event = Event)

g_1 <- sent_polarity + 
  geom_vline(xintercept = Time,
             linetype="dotted", size = 0.3) 

g_1 + geom_text(data = timeline,
              mapping = aes(x = Time, 
                            y = rep.int(48,7), 
                            label = Event),
              inherit.aes = FALSE,
              angle = 90,
              size = 2.5,
              hjust = 1)

g <- sent_index  + 
  geom_vline(xintercept = Time,
             linetype="dotted", size = 0.3) 

g + geom_text(data = timeline,
              mapping = aes(x = Time, 
                            y = rep.int(-10,7), 
                            label = Event),
              inherit.aes = FALSE,
              angle = 90,
              size = 2.5,
              hjust = 1)


```

```{r}

# key event timeline after covid outbrak

Time_1 <- c("2020-01-23","2020-01-27","2020-02-06",
          "2020-02-13","2020-02-25","2020-03-10")
Event_1 <- c("Lockdown of Wuhan","Li Keqiang visited Wuhan","Death of Li Wenliang",
           "Replacement of Wuhan Communist Party chief",
           "Partial lockdown of Italy",
           "Xi visited Wuhan")
timeline_1 <- data.frame(Time = Time_1, Event = Event_1)

g_p1 <- sent_polarity_COVID  + 
  geom_vline(xintercept = Time_1,
             linetype="dotted", size = 0.3) 

g_p1 + geom_text(data = timeline_1,
              mapping = aes(x = Time, 
                            y = rep.int(50,6), 
                            label = Event),
              inherit.aes = FALSE,
              angle = 90,
              size = 2.5,
              hjust = 1)


g_1 <- sent_index_COVID  + 
  geom_vline(xintercept = Time_1,
             linetype="dotted", size = 0.3) 

g_1 + geom_text(data = timeline_1,
              mapping = aes(x = Time, 
                            y = rep.int(-0.05,6), 
                            label = Event),
              inherit.aes = FALSE,
              angle = 90,
              size = 2.5,
              hjust = 1)

ggsave(file="sentiment_COVID_average_minimal.png", width=7.5, height=4, dpi=300)


```

New visualisation plan after office hour 
1. No. of post each day (done)
-> increase after covid & cor key events, social media vs official
2. sum of sentiment per day & each category of emotion
3. average: sum of sentiment / root (no. post) - ad hoc
4. range of post sentiment per day (done)


Remaining questions: why significantly less than before outbreak? how to measure sentiment in social media posts like "???"

# Topic modeling

Performed in Rstudio cloud since the local Rstudio can't display Chinese character in plots.

Identified sub topic for further analysis:
1. 李文亮
2. 方舱
3. 防护措施（propaganda）?
4. 武汉封城
5. Li & Xi visiting
6. removal of Wuhan official (~5days)?
7. 吹哨人/发哨人
Key words used for data filtering is available in the final report.

Qualitative study：
retweet of 人日， read original post

Weibo ID of offcial accounts:
人民数字 2527194860
人民日报数字传播 2713131601
人民日报评论 1846816274
人民日报新兴媒体版 1840358785
有数青年 6315203766


```{r}
library("tm")
library("topicmodels")
library("reshape2")
library("tidytext")
```

```{r}

# Construct document-term matrix ----
dtm <- text_count %>% cast_dtm(timestamp, word, n)
# definition of document-term matrix: https://en.wikipedia.org/wiki/Document-term_matrix
dim(dtm)
# 96k words across 183 dates

# Set number of topics
K <- 30

# Set random number generator seed, ensures that you get the same result if you start with that same seed each time you run the same process
set.seed(1122)

# compute the LDA model, inference via 1000 iterations of Gibbs sampling
m_tm <- LDA(dtm, K, method="Gibbs", control=list(iter = 500, verbose = 25))
saveRDS(m_tm, "sample2/model_topicmodel.rds")

## beta: How words map to topics
sum_tm_beta <- tidy(m_tm, matrix = "beta")

## gamma: How documents map on topics
sum_tm_gamma <- tidy(m_tm, matrix = "gamma") %>%
  rename("date" = "document")
 

```

## Visualisation

```{r}
# Topics EDA ----

sum_tm_gamma %>%
  ggplot(aes(x = gamma)) +
  geom_density() +
  facet_wrap(~topic) +
  labs(
    title = "Topic Modeling Descriptive Statistics",
    subtitle = "Distribution of gamma"
  )


```

## Find valuable topics for interpretation

1. top words of each topic
```{r , message=FALSE, warning=FALSE}

### Get top words associated with topics ----

TOP_N_WORD <- 10

topic_top_word <- sum_tm_beta %>%
  rename("word" = "term") %>%
  group_by(topic) %>%
  slice_max(beta, n = TOP_N_WORD) %>%
  arrange(topic, desc(beta))

write_csv(topic_top_word, "sample2/tm_topic_top_word.csv")

TOP_N_DAYS <- 10

topic_top_days <- sum_tm_gamma %>%
  group_by(topic) %>%
  slice_max(gamma, n = TOP_N_DAYS) %>%
  arrange(topic, desc(gamma))

### Visualization 1: Topics in bar charts ----

topic_top_word %>%
  mutate(word = reorder_within(word, beta, topic)) %>%
  ggplot(aes(y = word, x = beta)) +
  geom_bar(stat = "identity") +
  facet_wrap(~topic, scales = "free_y", ncol = 5) +
  scale_y_reordered() + # Very interesting function. Use with reorder_within
  labs(
    title = "Topic Modeling",
    subtitle = "Top words associated with each topic"
  )

ggsave(file="sample2/top_word.png", width=7.5, height=4, dpi=300)

```

## Topics in word cloud
```{r}

library("ggwordcloud")

topic_top_word %>%
  ggplot(aes(label = word, size = beta)) +
  ggwordcloud::geom_text_wordcloud() +
  scale_size_area(max_size = 8) +
  facet_wrap(~factor(topic)) +
  labs(
    title = "Topic Modeling: Top words associated with each topic"
  )

#save.image("topic_modeling1.RData")


ggsave(file="sample2/word_cloud_by_topc.png", width=7.5, height=4, dpi=300)

```

## Time series of gamma

```{r}

library(ggplot2)

# summary 
sum_tm_gamma %>%
  ggplot(aes(x = date,  y = gamma, group = 1)) + 
  geom_line() +
  facet_wrap(~topic) +
  labs(title = "Gamma by date") 

ggsave(file="sample2/gamma_by_topc.png", width=7.5, height=4, dpi=300)


```

1- increase
3- sudden decrease
5- partial sudden drop 
6- gradual dicrease

```{r}
# plot each topic
gamma_by_topic <- sum_tm_gamma %>% pivot_wider(names_from = "topic", values_from = "gamma", 
            values_fill = 0, names_prefix = "topic_")
# topic 3 
gamma_by_topic %>%
  ggplot(aes(x = date,  y = topic_3, group = 1)) + 
  geom_line() +
  labs(title = "Topic 3") + 
  scale_x_discrete(guide = guide_axis(check.overlap = TRUE))

ggsave(file="gamma_topic3.png", width=7.5, height=4, dpi=300)

# topic 5
gamma_by_topic %>%
  ggplot(aes(x = date,  y = topic_5, group = 1)) + 
  geom_line() +
  labs(title = "Topic 5:protective measurements ") + 
  scale_x_discrete(guide = guide_axis(check.overlap = TRUE))

ggsave(file="gamma_topic5.png", width=7.5, height=4, dpi=300)

# topic 6
gamma_by_topic %>%
  ggplot(aes(x = date,  y = topic_6, group = 1)) + 
  geom_line() +
  labs(title = "Topic 6") + 
  scale_x_discrete(guide = guide_axis(check.overlap = TRUE))

# topic 1
gamma_by_topic %>%
  ggplot(aes(x = date,  y = topic_8, group = 1)) + 
  geom_line() +
  labs(title = "Topic 1") + 
  scale_x_discrete(guide = guide_axis(check.overlap = TRUE))

ggsave(file="gamma_topic1.png", width=7.5, height=4, dpi=300)

```

1 & 2 - covid cases
5 & 8 - protective measurements
7 - 正能量口号
9 - international debate
